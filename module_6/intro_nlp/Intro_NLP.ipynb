{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"serif\" size=\"6\" color=\"scarlet\">Natural Language Processing</font>\n",
    "\n",
    "It is a field in machine learning/deep learning that deals with understanding, analyzing, manipulating and generating language. Humans communicate through language on multiple mediums these days. It gets complicated. There is context, intonation, inflection and body language. The first major advancement in machine language processing was in 1950 when Alan Turing published \"Computing Machinery and Intelligence\". This paper establsihed the Turing Test, a criterion for how well a computer could impersonate a human. In 1957, Noam Chomsky's paper on Syntactic Structures revolutionized our understanding of linguistics. But a few decades passed without any real progress. It wasn't until the late 80's when ML algorithms were introduced that NLP showed real promise.\n",
    "\n",
    "        \n",
    "\n",
    "_NLP is not Neuro-linguistic programming(pseuodo-science - think changing behavior through hypnosis). Natural Language Understanding is similar to NLP but a bit different. NLP focuses on turning unstructured data into structured data. NLU is focused on content or sentiment analysis._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font face=\"script\" size=\"4\">\"Learn a language and you'll avoid a war\"-Arab proverb</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font face=\"script\" size=\"6\" color=\"scarlet\">Learning Objectives</font>\n",
    " - Understand what NLP is and how it is being used today to solve problems\n",
    " - Understand Regex and how its used to pattern match and filter\n",
    " - Understand common feature engineering techniques like stemming, lemming and bigrams\n",
    " - Understand POS tagging and parse trees "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font face=\"script\" size=\"6\" color=\"scarlet\">NLP in the Real World</font>\n",
    " \n",
    " Lots of everyday things we take for granted rely completely on NLP to function. Spell check and auto-complete, voice recognition/texting, spam filters, search engines, Siri/Alexa, google translate.\n",
    " \n",
    " - [AI having a convo](https://youtu.be/WnzlbyTZsQY)\n",
    " - [Summarize text](https://smmry.com/)\n",
    " - [Jennings vs. Watson](https://www.ted.com/talks/ken_jennings_watson_jeopardy_and_me_the_obsolete_know_it_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"serif\" size=\"4\">Definitions</font>\n",
    "<font face=\"serif\" size=\"4\"> \n",
    "* **Corpus/corpora** - a collection of written texts\n",
    "* **Linguistics** - the scientific study of language. Its form, meaning and context\n",
    "* **Information theory** - the study of how information is stored, quantified and communicated \n",
    "* **Morphology** - the study of words their formation and their relationship to other words\n",
    "* **Syntax** - the study of the rules that govern a language \n",
    "* **Semantics** - the study of the philosophical meaning of a language \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('job_scrape6.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"script\" size=\"6\" color=\"scarlet\">Preprocessing</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"serif\" size=\"4\">Regular Expressions</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](regex_cheat_sheet.png)\n",
    "\n",
    "<a href=\"https://www.debuggex.com/cheatsheet/regex/python\">Regex Cheatsheet</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting rid of upper cases. This avoids having multiple copies of the same words \n",
    "df['lower_desc'] = df['description'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "df['lower_desc'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing punctuation. It helps us reduce the size of the data \n",
    "df['lower_desc'] = df['lower_desc'].str.replace('[^\\w\\s]','')\n",
    "df['lower_desc'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"serif\" size=\"4\">**Natural Language Tool Kit (NLTK)** - popular open-source python package for dealing with text data</font>\n",
    "\n",
    "[documentation](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"serif\" size=\"4\">**Tokenization** -splitting the whole into pieces or tokens. A word is a token in a whole sentence. A sentence is a token in a whole paragraph. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to split a sentence into a list of words \n",
    "word_tokenize('I am Jon Snow of House Stark.') #how do you pass something like a df.column? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"serif\" size=\"4\">**Stop Words Removal** - words that don't contribute to the significance or meaning of a document </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['char_count'] = df['description'].str.len() #how many characters do we have in description? \n",
    "df[['description','char_count']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['char_count'].sum() #including spaces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many stop words do we have? \n",
    "df['stopwords'] = df['description'].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "df[['description','stopwords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stopwords'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords \n",
    "df['lower_desc'] = df['lower_desc'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "df['lower_desc'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most frequent and least frequent words \n",
    "freq = pd.Series(' '.join(df['lower_desc']).split()).value_counts()[:20]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['no_zip_location'] = df['location'].str.replace('\\d+', '')\n",
    "#locations = df['location'].str.split(' ', n=2, expand=True)\n",
    "#df['no_zip_location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_str = ' '.join(df['lower_desc'].tolist())\n",
    "print(desc_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(desc_str) #back to tokenizing \n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"serif\" size=\"4\">**POS Tagging** - tagging each word in the corpus with a part of speech.\n",
    "- CC coordinating conjunction\n",
    "- CD cardinal digit\n",
    "- DT determiner\n",
    "- EX existential there (like: “there is” … think of it like “there exists”)\n",
    "- FW foreign word\n",
    "- IN preposition/subordinating conjunction\n",
    "- JJ adjective ‘big’\n",
    "- JJR adjective, comparative ‘bigger’\n",
    "- JJS adjective, superlative ‘biggest’\n",
    "- LS list marker 1)\n",
    "- MD modal could, will\n",
    "- NN noun, singular ‘desk’\n",
    "- NNS noun plural ‘desks’\n",
    "- NNP proper noun, singular ‘Harrison’\n",
    "- NNPS proper noun, plural ‘Americans’\n",
    "- PDT predeterminer ‘all the kids’\n",
    "- POS possessive ending parent’s\n",
    "- PRP personal pronoun I, he, she </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_pos = nltk.pos_tag(tokens)\n",
    "pos_df = pd.DataFrame(tokens_pos, columns = ('word','POS'))\n",
    "pos_sum = pos_df.groupby('POS', as_index=False).count() # group by POS tags\n",
    "pos_sum.sort_values(['word'], ascending=[False]) # in descending order of number of words per tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting just the nouns\n",
    "filtered_pos = [ ]\n",
    "for one in tokens_pos:\n",
    "    if one[1] == 'NN' or one[1] == 'NNS' or one[1] == 'NNP' or one[1] == 'NNPS':\n",
    "        filtered_pos.append(one)\n",
    "print (len(filtered_pos))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the 100 most common nouns\n",
    "fdist_pos = nltk.FreqDist(filtered_pos)\n",
    "top_100_words = fdist_pos.most_common(100)\n",
    "print(top_100_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_df = pd.DataFrame(top_100_words, columns = ('pos','count'))\n",
    "top_words_df['Word'] = top_words_df['pos'].apply(lambda x: x[0]) # split the tuple of POS\n",
    "top_words_df = top_words_df.drop('pos', 1) # drop the previous column\n",
    "top_words_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,18))\n",
    "top_words_df.sort_values(by='count').plot.barh(x='Word',\n",
    "                      y='count',\n",
    "                      ax=ax,\n",
    "                      color=\"purple\")\n",
    "\n",
    "ax.set_title(\"Common Words Found in DS Job Descriptions(Without Stop Words)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for n-grams \n",
    "from textblob import TextBlob, Word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextBlob(desc_str).ngrams(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = ' '.join(top_words_df['Word'].tolist())\n",
    "print(type(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(word_counts)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"serif\" size=\"6\" color=\"scarlet\">Feature Engineering</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"serif\" size=\"4\">**Stemming** - a technique to remove affixes from a word and ending up with the stem. Play would be the stem of a word and the 'ing' in playing would be an affix. This process makes similar words more equal to each other. This way the algorithm only has to learn the stem of the word instead of the stem and all its variants.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"serif\" size=\"4\">**Lemmatization** - similar to stemming but it brings context to the words with morphological(words relationships to other words) analysis. A lemma is the base form of all its inflectional forms. Inflections are added to the stem of a word</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer \n",
    "porter = PorterStemmer() #instantiate\n",
    "lemma = WordNetLemmatizer() #instantiate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Porter Stemmer')\n",
    "print(porter.stem(\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lemmatizer')\n",
    "print(lemma.lemmatize(\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"serif\" size=\"4\">Count Vectorizing</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"serif\" size=\"4\">Term Frequency-Inverse Document Frequency (TF-IDF)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"serif\" size=\"4\" color=\"scarlet\">**_Term Frequency_** is calculated with the following formula:\n",
    "\n",
    "$$\\large Term\\ Frequency(t) = \\frac{number\\ of\\ times\\ t\\ appears\\ in\\ a\\ document} {total\\ number\\ of\\ terms\\ in\\ the\\ document} $$ \n",
    "\n",
    "**_Inverse Document Frequency_** is calculated with the following formula:\n",
    "\n",
    "$$\\large IDF(t) = log_e(\\frac{Total\\ Number\\ of\\ Documents}{Number\\ of\\ Documents\\ with\\ t\\ in\\ it})$$\n",
    "\n",
    "The **_TF-IDF_** value for a given word in a given document is just found by multiplying the two!</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
